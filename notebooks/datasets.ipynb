{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Datasets\n",
    "Datasets in TensorFlow are objects that represent a collection of data. They are used to feed data into a model for training or inference. Datasets can be created from a variety of sources such as NumPy arrays, Python generators, CSV files, and TFRecord files. Datasets can be transformed and manipulated using a variety of methods to prepare the data for training. Datasets can be iterated over using a for loop or by using the `iter` method to create an iterator. Datasets can be batched, shuffled, and repeated to create a data pipeline that feeds data into a model."
   ],
   "metadata": {
    "id": "N-oWjFu6OMD2"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating Datasets\n",
    "You can create a dataset by either choosing a source like a tensor, csv file, or others. Or you create a dataset from another dataset. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# create a dataset from a tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "list(dataset.as_numpy_iterator())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QaL8BLlgOGbY",
    "outputId": "0655163e-9b79-45b8-a661-0176789ac164",
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.516628Z",
     "start_time": "2024-12-23T03:11:16.504690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.543676Z",
     "start_time": "2024-12-23T03:11:16.532561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a dataset from a numpy array\n",
    "import numpy as np\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "list(dataset.as_numpy_iterator())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.567299Z",
     "start_time": "2024-12-23T03:11:16.556696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = tf.data.Dataset.range(12)\n",
    "list(dataset.as_numpy_iterator())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.590317Z",
     "start_time": "2024-12-23T03:11:16.579867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Once you have obtained a dataset, you can iterate over it using a for loop\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.614127Z",
     "start_time": "2024-12-23T03:11:16.603068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A more complex example of features and labels\n",
    "features = tf.constant([[1, 2], [3, 4], [5, 6]])\n",
    "labels = tf.constant(['cat', 'dog', 'fox'])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "list(dataset.as_numpy_iterator())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1, 2]), b'cat'), (array([3, 4]), b'dog'), (array([5, 6]), b'fox')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.646680Z",
     "start_time": "2024-12-23T03:11:16.627660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a dataset from a CSV file\n",
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "\n",
    "# load as dataframe\n",
    "df = pd.read_csv(titanic_file)\n",
    "df.head()\n",
    "\n",
    "# create a dataset from a pandas dataframe\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "for line in dataset.take(1):\n",
    "    for key, value in line.items():\n",
    "        print(f\"{key:20s}: {value}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived            : 0\n",
      "sex                 : b'male'\n",
      "age                 : 22.0\n",
      "n_siblings_spouses  : 1\n",
      "parch               : 0\n",
      "fare                : 7.25\n",
      "class               : b'Third'\n",
      "deck                : b'unknown'\n",
      "embark_town         : b'Southampton'\n",
      "alone               : b'n'\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "id": "il1PjcG_DoWv",
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.859416Z",
     "start_time": "2024-12-23T03:11:16.659665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# bring in the mnist dataset. This is NOT a tf.data.Dataset object though\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "# load the mnist dataset and extract them as numpy arrays\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# now if you wanted to, you could create a dataset from the numpy arrays\n",
    "# for preprocessing\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transforming Datasets\n",
    "Datasets can be transformed and manipulated in a variety of ways. Some common transformations include batching, shuffling, and repeating the data. Here are some examples:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.884677Z",
     "start_time": "2024-12-23T03:11:16.874020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# batch the dataset: instead of iterating over individual items, you can iterate over batches of items\n",
    "# Reminder: batching is useful when you want to train on multiple examples at once\n",
    "# which can be more efficient and can take advantage of parallelism\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "batched_dataset = dataset.batch(3)\n",
    "for batch in batched_dataset:\n",
    "    print(batch.numpy())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[4 5 6]\n",
      "[7 8 9]\n",
      "[10]\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.909544Z",
     "start_time": "2024-12-23T03:11:16.897581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# batching when there is a tuple of features and labels\n",
    "features = tf.constant([[1, 2], [3, 4], [5, 6]])\n",
    "labels = tf.constant(['cat', 'dog', 'fox'])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "batched_dataset = dataset.batch(2)\n",
    "for features, labels in batched_dataset:\n",
    "    print(f\"features: {features.numpy()}, labels: {labels.numpy()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [[1 2]\n",
      " [3 4]], labels: [b'cat' b'dog']\n",
      "features: [[5 6]], labels: [b'fox']\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T04:24:31.380275Z",
     "start_time": "2024-12-23T04:24:28.742299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# combining transformations\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "dataset = dataset.shuffle(3).batch(3)\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 4]\n",
      "[5 6 2]\n",
      "[ 9 10  7]\n",
      "[8]\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.969085Z",
     "start_time": "2024-12-23T03:11:16.947527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mapping a function to the dataset. Useful if you need to transform the data to a different format too\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item.numpy())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Consuming files from a directory\n",
    "You can create a dataset from files in a directory. This is useful when you have a large number of files that you want to feed into a model. Here is an example of how to create a dataset from image files:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:16.986129Z",
     "start_time": "2024-12-23T03:11:16.980736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "\n",
    "# download the flower photos dataset\n",
    "flowers_root = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n",
    "# get the path to the directory\n",
    "flowers_root = pathlib.Path(flowers_root)\n",
    "flowers_root"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/kingk/.keras/datasets/flower_photos')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:17.139193Z",
     "start_time": "2024-12-23T03:11:16.999102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Preprocessing for image files\n",
    "\n",
    "# this will create a dataset of the file paths\n",
    "# the stars being added will\n",
    "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
    "\n",
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())\n",
    "\n",
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def parse_image(filename):\n",
    "  parts = tf.strings.split(filename, os.sep)\n",
    "  label = parts[-2]\n",
    "\n",
    "  image = tf.io.read_file(filename)\n",
    "  image = tf.io.decode_jpeg(image)\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  image = tf.image.resize(image, [128, 128])\n",
    "  return image, label\n",
    "\n",
    "# now, lets create a dataset of the images and labels\n",
    "images_ds = list_ds.map(parse_image)\n",
    "for image, label in images_ds.take(1):\n",
    "    print(f\"image shape: {image.shape}, label: {label}\")\n",
    "\n",
    "# now you can batch and shuffle the dataset\n",
    "images_ds = images_ds.batch(32).shuffle(buffer_size=1000)\n",
    "\n",
    "# split into training and test\n",
    "train_size = int(0.7 * len(images_ds))\n",
    "train_ds = images_ds.take(train_size)\n",
    "test_ds = images_ds.skip(train_size)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'C:\\\\Users\\\\kingk\\\\.keras\\\\datasets\\\\flower_photos\\\\tulips\\\\14957470_6a8c272a87_m.jpg'\n",
      "b'C:\\\\Users\\\\kingk\\\\.keras\\\\datasets\\\\flower_photos\\\\tulips\\\\15052586652_56a82de133_m.jpg'\n",
      "b'C:\\\\Users\\\\kingk\\\\.keras\\\\datasets\\\\flower_photos\\\\sunflowers\\\\145303599_2627e23815_n.jpg'\n",
      "b'C:\\\\Users\\\\kingk\\\\.keras\\\\datasets\\\\flower_photos\\\\sunflowers\\\\18097401209_910a46fae1_n.jpg'\n",
      "b'C:\\\\Users\\\\kingk\\\\.keras\\\\datasets\\\\flower_photos\\\\roses\\\\17051448596_69348f7fce_m.jpg'\n",
      "image shape: (128, 128, 3), label: b'tulips'\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using Datasets with Keras\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T03:11:17.552056300Z",
     "start_time": "2024-12-23T03:07:11.125834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# load mnist from keras\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "# load the mnist dataset and extract them as numpy arrays\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create a dataset from the numpy arrays\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# preprocess the data\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_image)\n",
    "test_dataset = test_dataset.map(preprocess_image)\n",
    "\n",
    "# shuffle the data\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024)\n",
    "\n",
    "# split the training dataset into training and validation\n",
    "validation_split = 0.2\n",
    "num_train = len(x_train)\n",
    "num_val = int(validation_split * num_train)\n",
    "\n",
    "val_dataset = train_dataset.take(num_val)\n",
    "train_dataset = train_dataset.skip(num_val)\n",
    "\n",
    "# batch the data after splitting\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# prefetching will speed up the training process by prefetching the next batch while the current batch is being processed\n",
    "# that way the pipeline is always full and the data is always ready to be used.\n",
    "# AUTOTUNE will let tensorflow determine the optimal prefetch size.\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# create an improved model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Add early stopping to prevent overfitting\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=25,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# plot the training loss and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss_values, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'r.-', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# plot the training accuracy and validation accuracy\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, acc_values, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'r.-', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1972, in test_function  *\n        return step_function(self, iterator)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1956, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1944, in run_step  **\n        outputs = model.test_step(data)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1853, in test_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1179, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\metrics\\accuracy_metrics.py\", line 462, in sparse_categorical_accuracy\n        if matches.shape.ndims > 1 and matches.shape[-1] == 1:\n\n    TypeError: '>' not supported between instances of 'NoneType' and 'int'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[88], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m loss, accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoss :\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy :\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
      "File \u001B[1;32mD:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filed6v6t22z.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: in user code:\n\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1972, in test_function  *\n        return step_function(self, iterator)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1956, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1944, in run_step  **\n        outputs = model.test_step(data)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1853, in test_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1179, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\metrics\\base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"D:\\Development\\AI\\deeplearning-tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\metrics\\accuracy_metrics.py\", line 462, in sparse_categorical_accuracy\n        if matches.shape.ndims > 1 and matches.shape[-1] == 1:\n\n    TypeError: '>' not supported between instances of 'NoneType' and 'int'\n"
     ]
    }
   ],
   "execution_count": 88
  }
 ]
}
